{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43, 47,  3, 69, 73, 25, 61, 27, 16,  0,  0,  0,  4,  3, 69, 69, 19,\n",
       "       27, 18,  3,  2, 10, 60, 10, 25,  5, 27,  3, 61, 25, 27,  3, 60, 60,\n",
       "       27,  3, 60, 10, 35, 25, 57, 27, 25, 44, 25, 61, 19, 27, 14,  7, 47,\n",
       "        3, 69, 69, 19, 27, 18,  3,  2, 10, 60, 19, 27, 10,  5, 27, 14,  7,\n",
       "       47,  3, 69, 69, 19, 27, 10,  7, 27, 10, 73,  5, 27, 36, 50,  7,  0,\n",
       "       50,  3, 19, 58,  0,  0, 81, 44, 25, 61, 19, 73, 47, 10,  7], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps # 한 step에 RNN cell에 입력으로 들어가는 데이터 slice의 크기\n",
    "    n_batches = int(len(chars) / slice_size) # 데이터의 전체 length를 slice_size로 나눈 만큼 batch 실행\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    # bach_size가 10이라면 전체 데이터를 10개로 나눠서 stack하니 10 row가 된다. \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    # 왜냐하면 x 전체 길이는 n_batches*batch_size*num_steps이므로 \n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    # |=============================================|===============|\n",
    "    # |                 training_x                  |     val_x     |\n",
    "    # |=============================================|===============|\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43, 47,  3, 69, 73, 25, 61, 27, 16,  0,  0,  0,  4,  3, 69, 69, 19,\n",
       "        27, 18,  3,  2, 10, 60, 10, 25,  5, 27,  3, 61, 25, 27,  3, 60, 60,\n",
       "        27,  3, 60, 10, 35, 25, 57, 27, 25, 44, 25, 61, 19, 27, 14,  7],\n",
       "       [27,  3,  2, 27,  7, 36, 73, 27, 28, 36, 10,  7, 28, 27, 73, 36, 27,\n",
       "         5, 73,  3, 19, 15,  8, 27,  3,  7,  5, 50, 25, 61, 25,  6, 27, 30,\n",
       "         7,  7,  3, 15, 27,  5,  2, 10, 60, 10,  7, 28, 15, 27,  1, 14],\n",
       "       [44, 10,  7, 58,  0,  0,  8, 31, 25,  5, 15, 27, 10, 73, 82,  5, 27,\n",
       "         5, 25, 73, 73, 60, 25,  6, 58, 27, 37, 47, 25, 27, 69, 61, 10, 34,\n",
       "        25, 27, 10,  5, 27,  2,  3, 28,  7, 10, 18, 10, 34, 25,  7, 73],\n",
       "       [ 7, 27,  6, 14, 61, 10,  7, 28, 27, 47, 10,  5, 27, 34, 36,  7, 44,\n",
       "        25, 61,  5,  3, 73, 10, 36,  7, 27, 50, 10, 73, 47, 27, 47, 10,  5,\n",
       "         0,  1, 61, 36, 73, 47, 25, 61, 27, 50,  3,  5, 27, 73, 47, 10],\n",
       "       [27, 10, 73, 27, 10,  5, 15, 27,  5, 10, 61, 39,  8, 27,  5,  3, 10,\n",
       "         6, 27, 73, 47, 25, 27, 36, 60,  6, 27,  2,  3,  7, 15, 27, 28, 25,\n",
       "        73, 73, 10,  7, 28, 27, 14, 69, 15, 27,  3,  7,  6,  0, 34, 61],\n",
       "       [27, 54, 73, 27, 50,  3,  5,  0, 36,  7, 60, 19, 27, 50, 47, 25,  7,\n",
       "        27, 73, 47, 25, 27,  5,  3,  2, 25, 27, 25, 44, 25,  7, 10,  7, 28,\n",
       "        27, 47, 25, 27, 34,  3,  2, 25, 27, 73, 36, 27, 73, 47, 25, 10],\n",
       "       [47, 25,  7, 27, 34, 36,  2, 25, 27, 18, 36, 61, 27,  2, 25, 15,  8,\n",
       "        27,  5, 47, 25, 27,  5,  3, 10,  6, 15, 27,  3,  7,  6, 27, 50, 25,\n",
       "         7, 73, 27,  1,  3, 34, 35, 27, 10,  7, 73, 36, 27, 73, 47, 25],\n",
       "       [57, 27,  1, 14, 73, 27,  7, 36, 50, 27,  5, 47, 25, 27, 50, 36, 14,\n",
       "        60,  6, 27, 61, 25,  3,  6, 10, 60, 19, 27, 47,  3, 44, 25, 27,  5,\n",
       "         3, 34, 61, 10, 18, 10, 34, 25,  6, 15, 27,  7, 36, 73, 27,  2],\n",
       "       [73, 27, 10,  5,  7, 82, 73, 58, 27, 37, 47, 25, 19, 82, 61, 25, 27,\n",
       "        69, 61, 36, 69, 61, 10, 25, 73, 36, 61,  5, 27, 36, 18, 27,  3, 27,\n",
       "         5, 36, 61, 73, 15,  0,  1, 14, 73, 27, 50, 25, 82, 61, 25, 27],\n",
       "       [27,  5,  3, 10,  6, 27, 73, 36, 27, 47, 25, 61,  5, 25, 60, 18, 15,\n",
       "        27,  3,  7,  6, 27,  1, 25, 28,  3,  7, 27,  3, 28,  3, 10,  7, 27,\n",
       "        18, 61, 36,  2, 27, 73, 47, 25, 27,  1, 25, 28, 10,  7,  7, 10]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이게 바로 한 step에서 사용하는 데이터 모양. batch_size(10개) 만큼의 row로 이루어져 있다. \n",
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "tmp_batch_x = get_batch([train_x], 50)\n",
    "\n",
    "for i, batch_x in enumerate(tmp_batch_x):\n",
    "    print(batch_x[0].shape)\n",
    "    if i == 3:\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    # 여기 inputs -> x_one_hot 과정에서 입력 데이터의 모양이 \n",
    "    # [batch_size, num_steps] -> [batch_size, num_steps, num_classes]로 바뀜\n",
    "    # 최종적으로 rnn에 입력으로 들어가는 모양이 이 모양이다. \n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    # 여기서 lstm_size는 hidden state의 크기 (The number of units in the hidden layers.)\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    # the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros.\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    # outputs.shape = [batch_size, num_steps, lstm_size]\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    # state.[0|1].[c|h].shape = [batch_size, lstm_size]\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    # seq_output.shape = [batch_size, num_steps, lstm_size]\n",
    "    # outputs가 list 형태가 아니라서 아무런 동작도 하지 않는다. tf.concat은 왜 넣은거지???\n",
    "    seq_output = tf.concat(outputs, axis=1) \n",
    "    # output.shape = [batch_size*num_steps, lstm_size]\n",
    "    # 여기에서 reshape 해주는 이유는 아래에서 softmax_cross_entropy_with_logits()에 넣어주는\n",
    "    # labels(y_reshpaped)와 차원을 맞춰주기 위해서이다. softmax_cross_entropy_with_logits()는\n",
    "    # [임의의 크기, one_hot_encoding된 벡터의 크기 즉, num_classes]를 입력으로 받아서 loss를 계산한다.\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    # softmax_w,_b를 이용해 최종 rnn_output을 [batch_size*num_steps, num_classes] 형태로 바꾼다.\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    # tensorflow로 gradients clipping 구현하는 방법. 기억해두자.\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    # 보통은 return 값으로 필요한 tensor들을 넘기는데 여기서는 namedtuple을 사용한게 재밌네.\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/35720 Training loss: 8.2694 2.2779 sec/batch\n",
      "Epoch 1/20  Iteration 2/35720 Training loss: 6.9651 2.2274 sec/batch\n",
      "Epoch 1/20  Iteration 3/35720 Training loss: 6.1943 2.1455 sec/batch\n",
      "Epoch 1/20  Iteration 4/35720 Training loss: 5.7727 2.5610 sec/batch\n",
      "Epoch 1/20  Iteration 5/35720 Training loss: 5.4278 2.2043 sec/batch\n",
      "Epoch 1/20  Iteration 6/35720 Training loss: 5.1725 2.5845 sec/batch\n",
      "Epoch 1/20  Iteration 7/35720 Training loss: 4.9682 2.6263 sec/batch\n",
      "Epoch 1/20  Iteration 8/35720 Training loss: 4.7973 2.6995 sec/batch\n",
      "Epoch 1/20  Iteration 9/35720 Training loss: 4.6628 2.1167 sec/batch\n",
      "Epoch 1/20  Iteration 10/35720 Training loss: 4.5492 2.1998 sec/batch\n",
      "Epoch 1/20  Iteration 11/35720 Training loss: 4.4568 2.2646 sec/batch\n",
      "Epoch 1/20  Iteration 12/35720 Training loss: 4.3728 2.3953 sec/batch\n",
      "Epoch 1/20  Iteration 13/35720 Training loss: 4.2942 2.2695 sec/batch\n",
      "Epoch 1/20  Iteration 14/35720 Training loss: 4.2276 2.3728 sec/batch\n",
      "Epoch 1/20  Iteration 15/35720 Training loss: 4.1724 2.1780 sec/batch\n",
      "Epoch 1/20  Iteration 16/35720 Training loss: 4.1223 2.1579 sec/batch\n",
      "Epoch 1/20  Iteration 17/35720 Training loss: 4.0767 2.5714 sec/batch\n",
      "Epoch 1/20  Iteration 18/35720 Training loss: 4.0346 2.2614 sec/batch\n",
      "Epoch 1/20  Iteration 19/35720 Training loss: 3.9992 2.1685 sec/batch\n",
      "Epoch 1/20  Iteration 20/35720 Training loss: 3.9609 2.1659 sec/batch\n",
      "Epoch 1/20  Iteration 21/35720 Training loss: 3.9297 2.2231 sec/batch\n",
      "Epoch 1/20  Iteration 22/35720 Training loss: 3.9018 2.4015 sec/batch\n",
      "Epoch 1/20  Iteration 23/35720 Training loss: 3.8750 2.1560 sec/batch\n",
      "Epoch 1/20  Iteration 24/35720 Training loss: 3.8477 2.1374 sec/batch\n",
      "Epoch 1/20  Iteration 25/35720 Training loss: 3.8249 2.2812 sec/batch\n",
      "Epoch 1/20  Iteration 26/35720 Training loss: 3.8018 2.2853 sec/batch\n",
      "Epoch 1/20  Iteration 27/35720 Training loss: 3.7803 2.3635 sec/batch\n",
      "Epoch 1/20  Iteration 28/35720 Training loss: 3.7602 2.2723 sec/batch\n",
      "Epoch 1/20  Iteration 29/35720 Training loss: 3.7414 2.9563 sec/batch\n",
      "Epoch 1/20  Iteration 30/35720 Training loss: 3.7221 2.4809 sec/batch\n",
      "Epoch 1/20  Iteration 31/35720 Training loss: 3.7061 3.2483 sec/batch\n",
      "Epoch 1/20  Iteration 32/35720 Training loss: 3.6883 2.2572 sec/batch\n",
      "Epoch 1/20  Iteration 33/35720 Training loss: 3.6721 2.2895 sec/batch\n",
      "Epoch 1/20  Iteration 34/35720 Training loss: 3.6575 2.4187 sec/batch\n",
      "Epoch 1/20  Iteration 35/35720 Training loss: 3.6427 2.6825 sec/batch\n",
      "Epoch 1/20  Iteration 36/35720 Training loss: 3.6299 2.4901 sec/batch\n",
      "Epoch 1/20  Iteration 37/35720 Training loss: 3.6165 2.1491 sec/batch\n",
      "Epoch 1/20  Iteration 38/35720 Training loss: 3.6044 2.1682 sec/batch\n",
      "Epoch 1/20  Iteration 39/35720 Training loss: 3.5928 2.2909 sec/batch\n",
      "Epoch 1/20  Iteration 40/35720 Training loss: 3.5803 2.6985 sec/batch\n",
      "Epoch 1/20  Iteration 41/35720 Training loss: 3.5709 2.3117 sec/batch\n",
      "Epoch 1/20  Iteration 42/35720 Training loss: 3.5598 2.6691 sec/batch\n",
      "Epoch 1/20  Iteration 43/35720 Training loss: 3.5494 2.5616 sec/batch\n",
      "Epoch 1/20  Iteration 44/35720 Training loss: 3.5372 2.3884 sec/batch\n",
      "Epoch 1/20  Iteration 45/35720 Training loss: 3.5263 2.2712 sec/batch\n",
      "Epoch 1/20  Iteration 46/35720 Training loss: 3.5180 2.1953 sec/batch\n",
      "Epoch 1/20  Iteration 47/35720 Training loss: 3.5096 2.2349 sec/batch\n",
      "Epoch 1/20  Iteration 48/35720 Training loss: 3.5007 3.1929 sec/batch\n",
      "Epoch 1/20  Iteration 49/35720 Training loss: 3.4928 2.2711 sec/batch\n",
      "Epoch 1/20  Iteration 50/35720 Training loss: 3.4836 2.6570 sec/batch\n",
      "Epoch 1/20  Iteration 51/35720 Training loss: 3.4755 3.3148 sec/batch\n",
      "Epoch 1/20  Iteration 52/35720 Training loss: 3.4656 3.2249 sec/batch\n",
      "Epoch 1/20  Iteration 53/35720 Training loss: 3.4599 2.9409 sec/batch\n",
      "Epoch 1/20  Iteration 54/35720 Training loss: 3.4524 2.3182 sec/batch\n",
      "Epoch 1/20  Iteration 55/35720 Training loss: 3.4453 2.5698 sec/batch\n",
      "Epoch 1/20  Iteration 56/35720 Training loss: 3.4376 2.7008 sec/batch\n",
      "Epoch 1/20  Iteration 57/35720 Training loss: 3.4301 2.7460 sec/batch\n",
      "Epoch 1/20  Iteration 58/35720 Training loss: 3.4239 2.9609 sec/batch\n",
      "Epoch 1/20  Iteration 59/35720 Training loss: 3.4158 3.5397 sec/batch\n",
      "Epoch 1/20  Iteration 60/35720 Training loss: 3.4081 2.7682 sec/batch\n",
      "Epoch 1/20  Iteration 61/35720 Training loss: 3.4014 2.5453 sec/batch\n",
      "Epoch 1/20  Iteration 62/35720 Training loss: 3.3947 2.4193 sec/batch\n",
      "Epoch 1/20  Iteration 63/35720 Training loss: 3.3871 3.0570 sec/batch\n",
      "Epoch 1/20  Iteration 64/35720 Training loss: 3.3792 3.2325 sec/batch\n",
      "Epoch 1/20  Iteration 65/35720 Training loss: 3.3717 3.2783 sec/batch\n",
      "Epoch 1/20  Iteration 66/35720 Training loss: 3.3638 3.0020 sec/batch\n",
      "Epoch 1/20  Iteration 67/35720 Training loss: 3.3566 2.2519 sec/batch\n",
      "Epoch 1/20  Iteration 68/35720 Training loss: 3.3502 2.1982 sec/batch\n",
      "Epoch 1/20  Iteration 69/35720 Training loss: 3.3439 2.2379 sec/batch\n",
      "Epoch 1/20  Iteration 70/35720 Training loss: 3.3369 2.3595 sec/batch\n",
      "Epoch 1/20  Iteration 71/35720 Training loss: 3.3305 2.3892 sec/batch\n",
      "Epoch 1/20  Iteration 72/35720 Training loss: 3.3244 2.3269 sec/batch\n",
      "Epoch 1/20  Iteration 73/35720 Training loss: 3.3173 2.1773 sec/batch\n",
      "Epoch 1/20  Iteration 74/35720 Training loss: 3.3117 2.1828 sec/batch\n",
      "Epoch 1/20  Iteration 75/35720 Training loss: 3.3054 2.5196 sec/batch\n",
      "Epoch 1/20  Iteration 76/35720 Training loss: 3.2994 2.1884 sec/batch\n",
      "Epoch 1/20  Iteration 77/35720 Training loss: 3.2927 2.2350 sec/batch\n",
      "Epoch 1/20  Iteration 78/35720 Training loss: 3.2876 2.2190 sec/batch\n",
      "Epoch 1/20  Iteration 79/35720 Training loss: 3.2808 2.2177 sec/batch\n",
      "Epoch 1/20  Iteration 80/35720 Training loss: 3.2762 2.6707 sec/batch\n",
      "Epoch 1/20  Iteration 81/35720 Training loss: 3.2695 2.2118 sec/batch\n",
      "Epoch 1/20  Iteration 82/35720 Training loss: 3.2632 2.1968 sec/batch\n",
      "Epoch 1/20  Iteration 83/35720 Training loss: 3.2575 2.2210 sec/batch\n",
      "Epoch 1/20  Iteration 84/35720 Training loss: 3.2520 2.5240 sec/batch\n",
      "Epoch 1/20  Iteration 85/35720 Training loss: 3.2473 2.2174 sec/batch\n",
      "Epoch 1/20  Iteration 86/35720 Training loss: 3.2417 2.1922 sec/batch\n",
      "Epoch 1/20  Iteration 87/35720 Training loss: 3.2371 2.1987 sec/batch\n",
      "Epoch 1/20  Iteration 88/35720 Training loss: 3.2313 2.1846 sec/batch\n",
      "Epoch 1/20  Iteration 89/35720 Training loss: 3.2256 2.4681 sec/batch\n",
      "Epoch 1/20  Iteration 90/35720 Training loss: 3.2202 2.1896 sec/batch\n",
      "Epoch 1/20  Iteration 91/35720 Training loss: 3.2141 2.1724 sec/batch\n",
      "Epoch 1/20  Iteration 92/35720 Training loss: 3.2081 2.1650 sec/batch\n",
      "Epoch 1/20  Iteration 93/35720 Training loss: 3.2025 2.2406 sec/batch\n",
      "Epoch 1/20  Iteration 94/35720 Training loss: 3.1965 2.4318 sec/batch\n",
      "Epoch 1/20  Iteration 95/35720 Training loss: 3.1915 2.1638 sec/batch\n",
      "Epoch 1/20  Iteration 96/35720 Training loss: 3.1859 2.1979 sec/batch\n",
      "Epoch 1/20  Iteration 97/35720 Training loss: 3.1820 2.1890 sec/batch\n",
      "Epoch 1/20  Iteration 98/35720 Training loss: 3.1779 2.4869 sec/batch\n",
      "Epoch 1/20  Iteration 99/35720 Training loss: 3.1729 2.1965 sec/batch\n",
      "Epoch 1/20  Iteration 100/35720 Training loss: 3.1694 2.2004 sec/batch\n",
      "Epoch 1/20  Iteration 101/35720 Training loss: 3.1641 2.1813 sec/batch\n",
      "Epoch 1/20  Iteration 102/35720 Training loss: 3.1602 2.1759 sec/batch\n",
      "Epoch 1/20  Iteration 103/35720 Training loss: 3.1555 2.5163 sec/batch\n",
      "Epoch 1/20  Iteration 104/35720 Training loss: 3.1516 2.1753 sec/batch\n",
      "Epoch 1/20  Iteration 105/35720 Training loss: 3.1464 2.1765 sec/batch\n",
      "Epoch 1/20  Iteration 106/35720 Training loss: 3.1416 2.2514 sec/batch\n",
      "Epoch 1/20  Iteration 107/35720 Training loss: 3.1368 2.5352 sec/batch\n",
      "Epoch 1/20  Iteration 108/35720 Training loss: 3.1319 2.2919 sec/batch\n",
      "Epoch 1/20  Iteration 109/35720 Training loss: 3.1271 2.1662 sec/batch\n",
      "Epoch 1/20  Iteration 110/35720 Training loss: 3.1223 2.2118 sec/batch\n",
      "Epoch 1/20  Iteration 111/35720 Training loss: 3.1183 2.1811 sec/batch\n",
      "Epoch 1/20  Iteration 112/35720 Training loss: 3.1137 2.6930 sec/batch\n",
      "Epoch 1/20  Iteration 113/35720 Training loss: 3.1099 2.2048 sec/batch\n",
      "Epoch 1/20  Iteration 114/35720 Training loss: 3.1061 2.2240 sec/batch\n",
      "Epoch 1/20  Iteration 115/35720 Training loss: 3.1014 2.1981 sec/batch\n",
      "Epoch 1/20  Iteration 116/35720 Training loss: 3.0968 2.6499 sec/batch\n",
      "Epoch 1/20  Iteration 117/35720 Training loss: 3.0928 2.3911 sec/batch\n",
      "Epoch 1/20  Iteration 118/35720 Training loss: 3.0894 2.2173 sec/batch\n",
      "Epoch 1/20  Iteration 119/35720 Training loss: 3.0858 2.1988 sec/batch\n",
      "Epoch 1/20  Iteration 120/35720 Training loss: 3.0818 2.3497 sec/batch\n",
      "Epoch 1/20  Iteration 121/35720 Training loss: 3.0783 2.3431 sec/batch\n",
      "Epoch 1/20  Iteration 122/35720 Training loss: 3.0756 2.2075 sec/batch\n",
      "Epoch 1/20  Iteration 123/35720 Training loss: 3.0725 2.2188 sec/batch\n",
      "Epoch 1/20  Iteration 124/35720 Training loss: 3.0681 2.1917 sec/batch\n",
      "Epoch 1/20  Iteration 125/35720 Training loss: 3.0635 2.4731 sec/batch\n",
      "Epoch 1/20  Iteration 126/35720 Training loss: 3.0602 2.3188 sec/batch\n",
      "Epoch 1/20  Iteration 127/35720 Training loss: 3.0560 2.4564 sec/batch\n",
      "Epoch 1/20  Iteration 128/35720 Training loss: 3.0525 2.2235 sec/batch\n",
      "Epoch 1/20  Iteration 129/35720 Training loss: 3.0487 2.4057 sec/batch\n",
      "Epoch 1/20  Iteration 130/35720 Training loss: 3.0448 2.3745 sec/batch\n",
      "Epoch 1/20  Iteration 131/35720 Training loss: 3.0408 2.7489 sec/batch\n",
      "Epoch 1/20  Iteration 132/35720 Training loss: 3.0372 3.2834 sec/batch\n",
      "Epoch 1/20  Iteration 133/35720 Training loss: 3.0330 2.6678 sec/batch\n",
      "Epoch 1/20  Iteration 134/35720 Training loss: 3.0290 2.4300 sec/batch\n",
      "Epoch 1/20  Iteration 135/35720 Training loss: 3.0251 2.2750 sec/batch\n",
      "Epoch 1/20  Iteration 136/35720 Training loss: 3.0216 2.2844 sec/batch\n",
      "Epoch 1/20  Iteration 137/35720 Training loss: 3.0188 2.7347 sec/batch\n",
      "Epoch 1/20  Iteration 138/35720 Training loss: 3.0149 2.8314 sec/batch\n",
      "Epoch 1/20  Iteration 139/35720 Training loss: 3.0117 2.9449 sec/batch\n",
      "Epoch 1/20  Iteration 140/35720 Training loss: 3.0077 2.8476 sec/batch\n",
      "Epoch 1/20  Iteration 141/35720 Training loss: 3.0041 2.7328 sec/batch\n",
      "Epoch 1/20  Iteration 142/35720 Training loss: 3.0004 2.2337 sec/batch\n",
      "Epoch 1/20  Iteration 143/35720 Training loss: 2.9963 2.2564 sec/batch\n",
      "Epoch 1/20  Iteration 144/35720 Training loss: 2.9930 2.7186 sec/batch\n",
      "Epoch 1/20  Iteration 145/35720 Training loss: 2.9891 2.9971 sec/batch\n",
      "Epoch 1/20  Iteration 146/35720 Training loss: 2.9854 2.7181 sec/batch\n",
      "Epoch 1/20  Iteration 147/35720 Training loss: 2.9818 2.2145 sec/batch\n",
      "Epoch 1/20  Iteration 148/35720 Training loss: 2.9784 2.2745 sec/batch\n",
      "Epoch 1/20  Iteration 149/35720 Training loss: 2.9743 2.6362 sec/batch\n",
      "Epoch 1/20  Iteration 150/35720 Training loss: 2.9707 2.7292 sec/batch\n",
      "Epoch 1/20  Iteration 151/35720 Training loss: 2.9673 2.6939 sec/batch\n",
      "Epoch 1/20  Iteration 152/35720 Training loss: 2.9637 2.5714 sec/batch\n",
      "Epoch 1/20  Iteration 153/35720 Training loss: 2.9602 2.9209 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ddfd5f91c8ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# new_state를 계속 갱신하며 initial_state로 세팅하여 rnn 작업 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 37\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/coupang/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/coupang/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/coupang/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/coupang/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/coupang/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "# max_to_keep 개수만큼의 최근 checkpoint를 유지한다. \n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            # new_state를 계속 갱신하며 initial_state로 세팅하여 rnn 작업 수행\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b), # epoch 초반엔 noise가 비교적 클 것\n",
    "                  '{:.4f} sec/batch'.format((end-start))) # batch 당 시간 측정 방법\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                # ckpt 이름에 loss 값을 넣는거 좋은 아이디어인듯. 활용해보자.\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    '''\n",
    "    전체 vocab에서 발생 확률이 높은 top_n개 중에 랜덤으로 한 개를 골라서 해당 인덱스를 리턴\n",
    "    '''\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        # The를 초기 입력으로 넣어서 sampling 시작\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        # prime(여기서는 \"The \") 다음에 나올 character를 c에 담아두고 \n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        # 이것을 시작으로 sample 데이터 생성\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Far$FF9*!hv9v!F9vur@FJKur@9F.9uF9Fv$9*uF9r.h9uAA9hv$9Fu $9h$u49.s9hv$9Q.!rh9!r9hv$9O.rh4!4!h!.r9.s9hv$9O. QAu!rh9!r9hv$9Fu $KFhur@$49*!hv9v! EKK61._9zr.*9v.*9'9@.r&h9FQu4$9v$49!h79'9Ourr.h9huz$9hv$9F_Ov9 _Oz9T4.hv$49h.9v! J69Fv$9Fu!@9F!,r!s!OurO$FJ9ur@9*v!Ov9hv$o9*$4$9u9FQu4$Ao9Fuh$@9s.49hv$!49O4$uh_4$9h.9uFz9hv$KQ4.Q$4J9ur@9*vuh9!r9hv$!49 .r$o9h! $9hvuh9vu@KO. $9uT._h9!hJ69v$9Fu!@9ur@9Fh4!Oz!r,9hv$9F$O.r@9F !A$J9ur@9F$$!r,9hv$9huh$9uAA9hv$9OA$u49.s9v!F9v$u4h9*!hv9urohv!r,J9uF9hv4$$Kh4._TA$9.s9F. $hv!r,9h.9T$9!r@$$@9!r9u9O. QA!$rO$9hvuh9!hK*$4$9Fh!AA9hv$9Ou44!u,$E9:$9vu@9TuOz9uFz$@9v$49s4. 9v!F9suO$EKK2v$KQ4.T.h!O9Fh$Q9ur@9vuQQ!r$FF9.s9u9O. !r,9.r$9.s9v!FKT.@!A!huh!.rFE9:$9O._A@9r.h9FQ$uz9._h9h.9v! E9\"h9v!r9h! $J9F.9hv$9O4!Oz$49*!hv9*._A@9vuy$9T$$r9u9Q$.QA$J9v$9*uF9uh9hv$9Ov$Fh9ur@9v$49h.9u9Ov!4Ov$r9uFh! Qh!.rFJ9T_h9Fv!r!r,9vur@FJ9T_h9v$49s!r@Khuz!r,Kv! 9*uFKu9,$r_4$9hvuh9hv$o9vu@9uA4$F9uh9hv$K !r_h$E92v$9Q4.O_F$9.s9F.9*!hv9uFzu!rF9hvuh9Fv$9huz$9._h9.s9hv$9Q.F!h!.r9*$4$9Fh4.r,$49._hE9:$9*uF9r.hKurF*$4$@EKK6ev$r9'9*.r&h9Ou44o9hvuh9F. $9 uFh$4F9ur@9 o9v!,vJ9!rFh!rOh!y$9 $J69v$9Fu!rE9:$9*uF9hv$9h$r@$49uFh$rh!.rF9ur@9h.A@KTo9v! 9hv$9Q.A!$F9!r9v$49Fv$uhJ9hv$o9O._A@9FuoJ9ur@9*!hv._h9v!F9v$u4h9*!hv9u9Fh4ur,$J9Fv$9h.A@9v$4Ku9h$r@!._F9Q4.s$FF9uF9!h9uFzur@9h..z9hv$9F.r9h.A@9hv$9F.sh9uh9v! 9uA.r$9h.9hv$9Q4.Q.4h$@9v!F9s$$h9uh9hv$9FhuOz9ur@Khv$9Ou44oJ9v!F9suO$9!r9v!F9F!Fh$4bb!r9u9Fh4$r,$9v$9Fhuh$@9ur@9Fh$Q9ur@9O.44!OuA9ur@9O. !r,9h.9hv$9h! $K.s9!hJ9v$9h..z9hv$9Ov!A@4$rEKK6'9*!AA9hv!rz9hv$o9*._A@9r.h9huAz9h.9 $E6KK6evuh9*!AA9'9h$AA9o._76KK6R.9.s9hvuhJ69v$9urF*$4$@9ur@J9uF9v$9Fu!@9h.9v! 9ur.hv$49s4. 9uAA9h$r9ur@KvuAs9uF9uh9hv$9Q4!rO$FF9uh9hv$9 .4$KF!rO$4$J9ur@9Fv!rh!r,9ur@9Fvu $9ur@9urF*$4!r,9hvuh9Fv$9*uF9uAA9h.9F$$9v$4J9ur@9hvuh9Fv$9*uF9u9,!y$9.s9uF9hv$9Q$uFurh&F9$PO!h$@9uh9hv$9O4.QF9hvuh9hv$9h! $9hv$4$9*uF9F. $hv!r,9*$4$9O._4F$9ur@9v!F9T4.hv$49ur@Kv_44o9.y$49v_FTur@9.s9v$4J9*!hv9u9F !A$9h.9v!F9Fh!Oz!r,9!rh!r_h!OuA9Q$.QA$E962vuh9*uF9!h9u9Fhu4hE9:!F9OuQu!4J9T_h9!r9hv$KQAuO$9ur@9v_r,4o\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i6000_l512_v1.350.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
